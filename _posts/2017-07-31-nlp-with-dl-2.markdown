---
layout: post
title:  "NLP with Deep Learning - Lecture 2"
date:   2017-07-31 09:10:00 +0800
categories: natural language processing posts
---

本篇是学习斯坦福公开课"Natural Language Processing with Deep Learning"时的笔记。



包含leture2的内容。

---

### 目录

1. Word2Vec的含义

2. 两种实现方法

   2.1. Skip-Gram

   2.2. CBOW

3. 两种优化方法

   3.1. Negative Sampling

   3.2. Hierarchical Softmax

4. 两种求导

   4.1. Naive Softmax bp

   4.2. Negative Sampling bp

----

### 1. Word2Vec的含义

一个单词，神经网络理解不了，需要人转换成数字再喂给它。最naive的方式就是one-hot，但是太过于稀疏，不好。所以在改进一下，把one-hot进一步压缩成一个dense vector。



word2vec算法就是根据上下文预测单词，从而获得词向量矩阵。



**预测单词的任务只是一个幌子，我们需要的结果并不是预测出来的单词，而是通过预测单词这个任务，不断更新着的参数矩阵weights。**



预测任务由一个简单的三层神经网络来完成，其中有两个参数矩阵$V$和$U$，$V\in\mathbb{R}^{D_h\times\mid W \mid}$，$U\in\mathbb{R}^{\mid W \mid\times D_h}$。



$V$是输入层到隐藏层的矩阵，又被称为look-up table（因为，输入的是one-hot向量，一个one-hot向量乘以一个矩阵相当于取了这个矩阵的其中一列。将其中的每一列看成是词向量）



$U$是隐藏层到输出层的矩阵，又被称为word representation matrix（将其中的每一行看成是词向量）



**最后需要的词向量矩阵是将两个词向量矩阵相加 = $V+U^T$，然后每一列就是词向量。**



### 2. 两种实现方法

#### 2.1. Skip-Gram

**训练任务：根据中心词，预测出上下文词** 



输入：一个中心词（center word，$x \in \mathbb{R}^{\mid W \mid \times 1}$）



参数：一个look up table $V\in\mathbb{R}^{D_h\times\mid W \mid}$，一个word representation matrix $U\in\mathbb{R}^{\mid W \mid\times D_h}$



输出：$T$个上下文词（context word，$\hat{y} \in \mathbb{R}^{\mid W \mid\times 1}$）



损失函数：cross-entropy - $J_t(\theta) =  y\log{\hat{y}}$



详细步骤：



$$
v_c=Vx \in \mathbb{R}^{D_h\times1}
$$

$$
z=Uv_c \in \mathbb{R}^{\mid W \mid \times 1}
$$

$$
\hat{y}=softmax(z) \in \mathbb{R}^{\mid W \mid \times 1}
$$

$$
J_t(\theta)=y\log{\hat{y}}
$$

$$
J(\theta) = \frac{1}{T}\sum^{T}_{t=1} J_t(\theta)
$$



Skip-Gram步骤图：



![nlp_with_dl_2_slice_1]({{ site.baseurl }}/img/in-post/nlp_with_dl_lecture2/nlp_with_dl_2_slice_1.jpg)



#### 2.2. CBOW

与Skip-Gram相反，是通过完成**上下文词预测中心词的任务**来训练词向量的。



**训练任务：根据上下文词，预测出中心词**  



输入：$T$个上下文词（context word，$x\in \mathbb{R}^{\mid W \mid\times 1}$）



参数：一个look up table $V\in\mathbb{R}^{D_h\times\mid W \mid}$，一个word representation matrix $U\in\mathbb{R}^{\mid W \mid\times D_h}$



输出：一个中心词（center word，$\hat{y} \in \mathbb{R}^{\mid W \mid\times 1}$）



损失函数：cross-entropy - $J_t(\theta) = y\log{\hat{y}}$ 



详细步骤：



$$
v_{o_t}=V \cdot x_t \in \mathbb{R}^{D_h\times1}
$$

$$
v_o=\sum^{T}_{t=1}v_{o_t}
$$

$$
z=Uv_o \in \mathbb{R}^{\mid W \mid \times 1}
$$

$$
\hat{y}=softmax(z) \in \mathbb{R}^{\mid W \mid \times 1}
$$

$$
J(\theta)=J_t(\theta) = y\log{\hat{y}}
$$



CBOW步骤图：



![nlp_with_dl_2_slice_2]({{ site.baseurl }}/img/in-post/nlp_with_dl_lecture2/nlp_with_dl_2_slice_2.jpg)



### 3. 两种优化方法

#### 3.1. Negative Sampling 

以下内容基于Skip-Gram模型。CBOW同理。



原始的方法中使用$softmax()$将$z$转化成$\hat{y}$。很明显地，$softmax()$公式的分母将整个corpus都遍历了一遍，这个计算量并不小：



$$
softmax(z)=\frac{z_i}{\sum^{\mid W \mid}_{j=1}z_j}
$$


代替的方法是“负采样”。即从一个特定分布中，随机取出$k$个words来代表整个corpus。$k$个words和中心词*(center word)*组成成为***noise pairs***，上下文词*(context words)*和中心词*(center word)*组合成为***true pairs***。



这样一来，需要改变loss function为：


$$
J_t(\theta)=-\log\hat{y_{i}}-\sum^{K}_{k=1}\log\hat{y_{k_{i}}}
$$


下标$i$表示正确类别，即one-hot vector中1所在的位置。



其中，使用$sigmoid()$将$z$转化成$\hat{y}$，同理$\hat{y_k}$：


$$
\hat{y_i}=sigmoid(z_i)=sigmoid(u_i^Tv_c)
$$

$$
\hat{y_k}=sigmoid(z_k)=sigmoid(u_k^Tv_c)
$$



注：$u_i=U[i]$，即$U$中第$i$行。



如果true pair出现的概率越大，noise pairs出现的概率越小，则这个loss会减小，满足loss的定义，所以可以把loss表示成如上公式。



#### 3.2. Hierarchical Softmax 

待续......



### 4. 两种求导

以下内容基于Skip-Gram模型。CBOW同理。



模型要学习的是两个参数矩阵$V$和$U$。



#### 4.1. Naive Softmax bp 

首先，对$V$求梯度其实就是对$v_c$求梯度，因为在forward过程中，参与loss计算的只有$V$中第c列。所以，只对$v_c$求导即可。



其次，对$U$求梯度，对$U$中每一个元素进行求导，因为在forward过程中，$U$中每一个元素参与loss计算(softmax中)。



Forward如下，只考虑一对pair：


$$
v_c=Vx 
$$

$$
z=Uv_c
$$

$$
\hat{y}=softmax(z)
$$

$$
J_t(\theta) =- y\log{\hat{y}}
$$

Back Propagate如下：



定义几个reused项：


$$
\delta_1=\frac{\partial{J_t}}{\partial{z}}=\hat{y}-y
$$


这一项很好求，$softmax(z)=\frac{z_i}{\sum^{\mid W \mid}_{j=1}z_j}$，先求$\frac{\partial{J_t}}{\partial{z_i}}$，再求$\frac{\partial{J_t}}{\partial{z_j}} $  $(j \neq i)$，然后组合成$\frac{\partial{J_t}}{\partial{z}}$。



然后对$U$进行求导：



$$
\frac{\partial{J_t}}{\partial{U}}=\frac{\partial{J_t}}{\partial{z}}\cdot\frac{\partial{z}}{\partial{U}}=\delta_1\cdot v_c^T
$$


提示：可以先大概的写出链式求导的几个项，然后根据维度进行组合。



接着对$V$进行求导：
$$
\frac{\partial{J_t}}{\partial{V}}=\frac{\partial{J_t}}{\partial{v_c}}=\frac{\partial{J_t}}{\partial{z}}\cdot\frac{\partial{z}}{\partial{v_c}}= U^T \cdot \delta_1
$$



#### 4.2. Negative Sampling bp 

首先，对$V$求梯度其实就是对$v_c$求梯度，因为在forward过程中，参与loss计算的只有$V$中第c列。所以，只对$v_c$求导即可。



其次，对$U$求梯度，对$u_o$和$u_k$求梯度，因为在forward过程中，参与loss计算的只有$u_o$和$u_k$。



待续......