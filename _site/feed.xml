<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Shenglei's Sketches</title>
    <description></description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Sat, 05 Nov 2016 19:17:00 +0800</pubDate>
    <lastBuildDate>Sat, 05 Nov 2016 19:17:00 +0800</lastBuildDate>
    <generator>Jekyll v3.3.0</generator>
    
      <item>
        <title>使用TensorFlow搭建最简单的全连接神经网络</title>
        <description>&lt;p&gt;前几天看了TensorFlow，今天把之前写的“三层全连接神经网络”重新写了一遍，然后发现了一个问题，最后自己解决啦～&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/ShengleiH/machine_learning/blob/master/naiveNN.py&quot;&gt;3层全连接网络完整代码&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;section&quot;&gt;神经网络的搭建&lt;/h4&gt;

&lt;h6 id=&quot;tensorflow&quot;&gt;导入tensorflow包：&lt;/h6&gt;
&lt;p&gt;这要是不导入怎么用啊？！&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import tensorflow as tf
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h6 id=&quot;mnist&quot;&gt;加载MNIST数据&lt;/h6&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets('MNIST_data', one_hot=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h6 id=&quot;section-1&quot;&gt;搭建空的网络框架&lt;/h6&gt;

&lt;p&gt;&lt;strong&gt;1. 定义好将来存放数据x和标签y的占位符placeholder&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;x = tf.placeholder(tf.float32, shape=[None, 784])
y_ = tf.placeholder(tf.float32, shape=[None, 10])
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;2. 定义好用来初始化weights和bias的函数，然后定义好变量W和b&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def init_weights(shape):
    initial = tf.truncated_normal(shape, stddev=0.1)
    return tf.Variable(initial)

def init_bias(shape):
    initial = tf.constant(0.1, shape=shape)
    return tf.Variable(initial)

W1 = init_weights([784, 100])
b1 = init_bias([100])
W2 = init_weights([100, 10])
b2 = init_bias([10])
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;3. 前向传播&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;使用relu函数作为激活函数，使用softmax作为分类器。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;y1 = tf.nn.relu(tf.matmul(x, W1) + b1)
y2 = tf.nn.softmax(tf.matmul(y1, W2) + b2)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;4. 反向传播&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;tensorflow已经封装好了back propagation。
由于分类器使用了softmax，所以这里的代价函数是&lt;strong&gt;交叉熵（cross-entropy）&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y2), reduction_indices=[1]))
train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;5. 模型评估&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;correct_prediction = tf.equal(tf.argmax(y2,1), tf.argmax(y_,1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h6 id=&quot;section-2&quot;&gt;训练模型&lt;/h6&gt;

&lt;p&gt;&lt;strong&gt;1. 开启事务session，初始化刚才定义的变量&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;session = tf.InteractiveSession()
session.run(tf.initialize_all_variables())
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;2. 赋值训练&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;for i in range(1000):
    batch = mnist.train.next_batch(100)
    train_step.run(feed_dict={x: batch[0], y_: batch[1]})
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h6 id=&quot;section-3&quot;&gt;测试模型&lt;/h6&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;print(accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels}))
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&quot;section-4&quot;&gt;我遇到的问题（解决了）&lt;/h4&gt;

&lt;p&gt;我发现的一个问题。&lt;/p&gt;

&lt;p&gt;每次在学习神经网络的时候，最搞不清楚的就是矩阵的维度，到底是nm？还是mn？这次在TensorFlow的代码中，也着实让我混乱了一把，因为它和我们理论学习中矩阵的维度是相反的！！！&lt;/p&gt;

&lt;p&gt;在这里我们搭建了一个3层的全连接神经网络：第一层输入层有784个nueron（即一张图片为28*28像素的）；第二层隐藏层有100个nueron（自己设置的，无所谓吧，应该）；第三层输出层有10个neuron（即标签的数量0–9，10个数字）&lt;/p&gt;

&lt;p&gt;于是在理论学习中，从输入层到隐藏层，我们会把&lt;strong&gt;输入图像矩阵&lt;/strong&gt;设置为&lt;strong&gt;784乘m&lt;/strong&gt;；&lt;strong&gt;权重矩阵&lt;/strong&gt;设置成&lt;strong&gt;100乘784&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;然而我们可以在代码中看到，TensorFlow是这样设置的：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;x = tf.placeholder(tf.float32, shape=[None, 784])
y_ = tf.placeholder(tf.float32, shape=[None, 10])
W1 = init_weights([784, 100])
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;也就是说，TensorFlow中把&lt;strong&gt;输入图像矩阵&lt;/strong&gt;设置为&lt;strong&gt;m乘784&lt;/strong&gt;；&lt;strong&gt;权重矩阵&lt;/strong&gt;设置成&lt;strong&gt;784乘100&lt;/strong&gt;,明显的，它的维度和我们理论学习中的是&lt;strong&gt;相反的&lt;/strong&gt;！！！&lt;/p&gt;

&lt;p&gt;那么我们在看&lt;strong&gt;模型评估&lt;/strong&gt;这里：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;correct_prediction = tf.equal(tf.argmax(y2,1), tf.argmax(y_,1))
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;argmax函数是把指定维度中最大的那一个的index返回。&lt;/p&gt;

&lt;p&gt;前面代码中定义的placeholders中y_的shape是[None, 10]，也就是说y_的第一维度在TensorFlow中表示数据的index，第二维度才是计算出来的10个label的值呀。而在模型评估中，用了tf.argmax(y_, 1)，就是说对y_的第一维度取最大值？！这不就错了？不应该使用第二维度吗？即tf.argmax(y_, 2)？&lt;/p&gt;

&lt;p&gt;然后我就试了一下tf.argmax(y_, 2)，报错了！！！说“这个维度的取值在[0,2)”，于是，我明白了！TensorFlow中维度是从0开始算的！！！&lt;strong&gt;很好，这相当程序员！！！&lt;/strong&gt;&lt;/p&gt;
</description>
        <pubDate>Sat, 05 Nov 2016 17:49:45 +0800</pubDate>
        <link>http://localhost:4000/maching-learning-tensorflow-1</link>
        <guid isPermaLink="true">http://localhost:4000/maching-learning-tensorflow-1</guid>
        
        
        <category>jekyll</category>
        
        <category>update</category>
        
      </item>
    
  </channel>
</rss>
