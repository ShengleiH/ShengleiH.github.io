<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>使用TensorFlow搭建最简单的全连接神经网络</title>
  <meta name="description" content="前几天看了TensorFlow，今天把之前写的“三层全连接神经网络”重新写了一遍，然后发现了一个问题，最后自己解决啦～">

  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="http://localhost:4000/maching-learning-tensorflow-1">
  <link rel="alternate" type="application/rss+xml" title="Shenglei's Sketches" href="http://localhost:4000/feed.xml">
</head>


  <body>
      <div class="outer">
        <header class="site-header">

  <div class="wrapper">

    <a class="site-title" href="/">Shenglei's Sketches</a>
    
  </div>

  <div class="header-badge">
    <a href="/">
        <img src="/assets/headshot.jpg" />
    </a>
  </div>
    
</header>


        <div class="page-content">
          <div class="wrapper">
            <article class="post card" itemscope itemtype="http://schema.org/BlogPosting">
 <div class="card-content">
  <header class="post-header">
    <h1 class="post-title" itemprop="name headline">使用TensorFlow搭建最简单的全连接神经网络</h1>
    <p class="post-meta"><time datetime="2016-11-05T17:49:45+08:00" itemprop="datePublished">Nov 5, 2016</time></p>
  </header>

  <div class="post-content" itemprop="articleBody">
    <p>前几天看了TensorFlow，今天把之前写的“三层全连接神经网络”重新写了一遍，然后发现了一个问题，最后自己解决啦～</p>

<p><a href="https://github.com/ShengleiH/machine_learning/blob/master/naiveNN.py">3层全连接网络完整代码</a></p>

<h4 id="section">神经网络的搭建</h4>

<h6 id="tensorflow">导入tensorflow包：</h6>
<p>这要是不导入怎么用啊？！</p>

<div class="highlighter-rouge"><pre class="highlight"><code>import tensorflow as tf
</code></pre>
</div>

<h6 id="mnist">加载MNIST数据</h6>

<div class="highlighter-rouge"><pre class="highlight"><code>from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets('MNIST_data', one_hot=True)
</code></pre>
</div>

<h6 id="section-1">搭建空的网络框架</h6>

<p><strong>1. 定义好将来存放数据x和标签y的占位符placeholder</strong></p>

<div class="highlighter-rouge"><pre class="highlight"><code>x = tf.placeholder(tf.float32, shape=[None, 784])
y_ = tf.placeholder(tf.float32, shape=[None, 10])
</code></pre>
</div>

<p><strong>2. 定义好用来初始化weights和bias的函数，然后定义好变量W和b</strong></p>

<div class="highlighter-rouge"><pre class="highlight"><code>def init_weights(shape):
    initial = tf.truncated_normal(shape, stddev=0.1)
    return tf.Variable(initial)

def init_bias(shape):
    initial = tf.constant(0.1, shape=shape)
    return tf.Variable(initial)

W1 = init_weights([784, 100])
b1 = init_bias([100])
W2 = init_weights([100, 10])
b2 = init_bias([10])
</code></pre>
</div>

<p><strong>3. 前向传播</strong></p>

<p>使用relu函数作为激活函数，使用softmax作为分类器。</p>

<div class="highlighter-rouge"><pre class="highlight"><code>y1 = tf.nn.relu(tf.matmul(x, W1) + b1)
y2 = tf.nn.softmax(tf.matmul(y1, W2) + b2)
</code></pre>
</div>

<p><strong>4. 反向传播</strong></p>

<p>tensorflow已经封装好了back propagation。
由于分类器使用了softmax，所以这里的代价函数是<strong>交叉熵（cross-entropy）</strong></p>

<div class="highlighter-rouge"><pre class="highlight"><code>cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y2), reduction_indices=[1]))
train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)
</code></pre>
</div>

<p><strong>5. 模型评估</strong></p>

<div class="highlighter-rouge"><pre class="highlight"><code>correct_prediction = tf.equal(tf.argmax(y2,1), tf.argmax(y_,1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
</code></pre>
</div>

<h6 id="section-2">训练模型</h6>

<p><strong>1. 开启事务session，初始化刚才定义的变量</strong></p>

<div class="highlighter-rouge"><pre class="highlight"><code>session = tf.InteractiveSession()
session.run(tf.initialize_all_variables())
</code></pre>
</div>

<p><strong>2. 赋值训练</strong></p>

<div class="highlighter-rouge"><pre class="highlight"><code>for i in range(1000):
    batch = mnist.train.next_batch(100)
    train_step.run(feed_dict={x: batch[0], y_: batch[1]})
</code></pre>
</div>

<h6 id="section-3">测试模型</h6>

<div class="highlighter-rouge"><pre class="highlight"><code>print(accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels}))
</code></pre>
</div>

<h4 id="section-4">我遇到的问题（解决了）</h4>

<p>我发现的一个问题。</p>

<p>每次在学习神经网络的时候，最搞不清楚的就是矩阵的维度，到底是nm？还是mn？这次在TensorFlow的代码中，也着实让我混乱了一把，因为它和我们理论学习中矩阵的维度是相反的！！！</p>

<p>在这里我们搭建了一个3层的全连接神经网络：第一层输入层有784个nueron（即一张图片为28*28像素的）；第二层隐藏层有100个nueron（自己设置的，无所谓吧，应该）；第三层输出层有10个neuron（即标签的数量0–9，10个数字）</p>

<p>于是在理论学习中，从输入层到隐藏层，我们会把<strong>输入图像矩阵</strong>设置为<strong>784乘m</strong>；<strong>权重矩阵</strong>设置成<strong>100乘784</strong>。</p>

<p>然而我们可以在代码中看到，TensorFlow是这样设置的：</p>

<div class="highlighter-rouge"><pre class="highlight"><code>x = tf.placeholder(tf.float32, shape=[None, 784])
y_ = tf.placeholder(tf.float32, shape=[None, 10])
W1 = init_weights([784, 100])
</code></pre>
</div>

<p>也就是说，TensorFlow中把<strong>输入图像矩阵</strong>设置为<strong>m乘784</strong>；<strong>权重矩阵</strong>设置成<strong>784乘100</strong>,明显的，它的维度和我们理论学习中的是<strong>相反的</strong>！！！</p>

<p>那么我们在看<strong>模型评估</strong>这里：</p>

<div class="highlighter-rouge"><pre class="highlight"><code>correct_prediction = tf.equal(tf.argmax(y2,1), tf.argmax(y_,1))
</code></pre>
</div>

<p>argmax函数是把指定维度中最大的那一个的index返回。</p>

<p>前面代码中定义的placeholders中y_的shape是[None, 10]，也就是说y_的第一维度在TensorFlow中表示数据的index，第二维度才是计算出来的10个label的值呀。而在模型评估中，用了tf.argmax(y_, 1)，就是说对y_的第一维度取最大值？！这不就错了？不应该使用第二维度吗？即tf.argmax(y_, 2)？</p>

<p>然后我就试了一下tf.argmax(y_, 2)，报错了！！！说“这个维度的取值在[0,2)”，于是，我明白了！TensorFlow中维度是从0开始算的！！！<strong>很好，这相当程序员！！！</strong></p>

  </div>
</div>
</article>

          </div>
        </div>

        <footer class="site-footer">

  <div class="wrapper">

    <span class="footer-heading">Shenglei's Sketches</span>

  </div>

</footer>

      </div>
  </body>
  
</html>
